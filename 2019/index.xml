<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>2019 on compulsory thinking</title>
    <link>https://opssec.in/2019/</link>
    <description>Recent content in 2019 on compulsory thinking</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 28 Dec 2019 23:00:00 +0000</lastBuildDate><atom:link href="https://opssec.in/2019/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Kubernetes Networking 101</title>
      <link>https://opssec.in/2019/1228/</link>
      <pubDate>Sat, 28 Dec 2019 23:00:00 +0000</pubDate>
      
      <guid>https://opssec.in/2019/1228/</guid>
      <description>Networking The first and easiest way to access the Pod container port is via port-forward command:
$ kubectl port-forward &amp;lt;pod_name&amp;gt; &amp;lt;local_port&amp;gt;:&amp;lt;remote_port&amp;gt;
Take a look on the dynamic port-forwarder.
Service An abstract way to expose an application running on a set of Pods as a network service.
a Service is an abstraction which defines a logical set of Pods and a policy by which to access them . The set of Pods targeted by a Service is usually determined by a selector.</description>
    </item>
    
    <item>
      <title>Kubernetes Deployment Workloads</title>
      <link>https://opssec.in/2019/12282/</link>
      <pubDate>Sat, 28 Dec 2019 11:00:00 +0000</pubDate>
      
      <guid>https://opssec.in/2019/12282/</guid>
      <description>ReplicaSets A ReplicaSetâ€™s purpose is to maintain a stable set of replica Pods running at any given time. So, starting with the ReplicaSetSpec.
// ReplicaSetSpec is the specification of a ReplicaSet. type ReplicaSetSpec struct { // Replicas is the number of desired replicas. // More info: https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/#what-is-a-replicationcontroller Replicas *int32 `json:&amp;#34;replicas,omitempty&amp;#34;` // Minimum number of seconds for which a newly created pod should be ready without any of its container crashing, for it to be considered available.</description>
    </item>
    
    <item>
      <title>Kubernetes Container API</title>
      <link>https://opssec.in/2019/12283/</link>
      <pubDate>Sat, 28 Dec 2019 10:00:00 +0000</pubDate>
      
      <guid>https://opssec.in/2019/12283/</guid>
      <description>Container from PodSpec The abstraction of the container is find inside a list of Container objects inside the PodSpec, it means the possibility to create multiple containers in a unique Pod.
// A single application container that you want to run within a pod. type Container struct { // Name of the container specified as a DNS_LABEL. Name string `json:&amp;#34;name&amp;#34;` // Docker image name. // More info: https://kubernetes.io/docs/concepts/containers/images Image string `json:&amp;#34;image,omitempty&amp;#34;` // Entrypoint array.</description>
    </item>
    
    <item>
      <title>Pod specification</title>
      <link>https://opssec.in/2019/1227/</link>
      <pubDate>Fri, 27 Dec 2019 15:00:00 +0000</pubDate>
      
      <guid>https://opssec.in/2019/1227/</guid>
      <description>Introduction Lets take a look in the default Kind definition of the Pod, with this we can enumerate all the fields and attached capabilities of the Pod. For this post only the main Spec of the Pod object is going to be detailed with examples and tasks, we let the container configuration for other posts. Get after it.
POD diagram Pod struct
PS: The detailing and lab section above extracts text, insights and possible examples from the links in the Spec struct.</description>
    </item>
    
    <item>
      <title>Kubectl Walkthrough</title>
      <link>https://opssec.in/2019/1226/</link>
      <pubDate>Thu, 26 Dec 2019 15:00:00 +0000</pubDate>
      
      <guid>https://opssec.in/2019/1226/</guid>
      <description>Introduction This is swiss-knife of the Kubernetes operator, the kubectl is a tools for translating declarative resources in Kubernetes objects via the API, so the operator can load, remove or update workloads, configurations and more.
The main source of truth is the Kubectl book but this post will try to summarize and organize the main capabilities of the tool.
Kubectl debugging and API server Kubectl is a client for the master API server, you can confirm this fact settings the verbose to 8, and you can fetch both the request and the response.</description>
    </item>
    
    <item>
      <title>Kubernetes in Docker</title>
      <link>https://opssec.in/2019/1225/</link>
      <pubDate>Wed, 25 Dec 2019 22:00:00 +0000</pubDate>
      
      <guid>https://opssec.in/2019/1225/</guid>
      <description>Introduction This post is for discussing more about Kind, setup some cluster examples with some particular customizations, and exlain what components it brings up.
The initial topics are a summary from Kind quickstart
Installing To install Kind you should use:
$ brew install kind
Or from source:
$ GO111MODULE=&amp;#34;on&amp;#34; go get sigs.k8s.io/kind@v0.6.1
First steps To create a new cluster you must run:
# Default cluster name is kind $ kind create cluster Creating cluster &amp;#34;kind&amp;#34; # A new cluster with context named 2k8s $ kind create cluster --name 2k8s Creating cluster &amp;#34;2k8s&amp;#34;</description>
    </item>
    
    <item>
      <title>Sawtooth Cheatsheet</title>
      <link>https://opssec.in/2019/1215/</link>
      <pubDate>Sun, 15 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://opssec.in/2019/1215/</guid>
      <description>Installation Ubuntu For local ubuntu Installation is being used Ubuntu Xenial - 16.04 LTS, via vagrant
$ vagrant init ubuntu/xenial64 $ vagrant up
After the box is up we need to add the correct repository:
$ sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 8AA7AF1F1091A5FD $ sudo add-apt-repository &amp;#39;deb http://repo.sawtooth.me/ubuntu/1.0/stable xenial universe&amp;#39; $ sudo apt update
Install the package:
$ sudo apt install -y sawtooth
Docker Install Docker adding key and repo:</description>
    </item>
    
    <item>
      <title>Sawtooth Permissioning</title>
      <link>https://opssec.in/2019/1208/</link>
      <pubDate>Sun, 08 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://opssec.in/2019/1208/</guid>
      <description>Introduction Sawtooth has two different kinds of permissioning, the first is the transaction permissioning, it controls who (user/clients) can submit transactions and batches to a validator, the second is validator key permissioning, this controls which nodes can connect to the Sawtooth network, only available with on-chain settings.
For this debugging and testing scenario we will have 2 nodes:
version: &amp;#34;2.1&amp;#34; services: v1: image: hyperledger/sawtooth-validator:1.1 container_name: v1 command: bash expose: - 8000 v2: image: hyperledger/sawtooth-validator:1.</description>
    </item>
    
    <item>
      <title>Kubernetes Audit Sink</title>
      <link>https://opssec.in/2019/1127/</link>
      <pubDate>Wed, 27 Nov 2019 10:00:00 +0000</pubDate>
      
      <guid>https://opssec.in/2019/1127/</guid>
      <description>Introduction The idea of this post is to test the Audit dynamic backend with a KIND cluster.
Kubernetes has 4 stages:
RequestReceived - The stage for events generated as soon as the audit handler receives the request, and before it is delegated down the handler chain. ResponseStarted - Once the response headers are sent, but before the response body is sent. This stage is only generated for long-running requests (e.</description>
    </item>
    
    <item>
      <title>Kubernetes API client</title>
      <link>https://opssec.in/2019/1123/</link>
      <pubDate>Sat, 23 Nov 2019 10:00:00 +0000</pubDate>
      
      <guid>https://opssec.in/2019/1123/</guid>
      <description>Introduction This is an overview and notes on how to use the client-go Kubernetes API.
Kubernetes API resources Accessing via API The API is versioned for extensability propose, the levels are Alpha level (v1alpha1), Beta (b2beta3) stabel (v1).
You can notice the groups of the resources exists inside the version. To access the endpoints directly start the proxy with $ kubectl proxy --port=8080 $ curl http://localhost:8080/apis/ # To dig deeper in versions and existent resources $ kubectl api-resources &amp;amp;&amp;amp; kubectl api-version</description>
    </item>
    
    <item>
      <title>From Dev to PoET</title>
      <link>https://opssec.in/2019/11161/dev-poet/</link>
      <pubDate>Sat, 16 Nov 2019 01:00:00 +0000</pubDate>
      
      <guid>https://opssec.in/2019/11161/dev-poet/</guid>
      <description>Introduction DevMode is very cool for development of transaction processors, but when you go to production it don&amp;#39;t grant a correct fork resolution of your chain, and you can end up with different HEAD in the nodes.
Some Byzantine Fault Tolerant protocols were created to fix this problem, in version 1.0.5 we have the famous PoET, when used with SGX (intel protection) it can guarantee the BFT characteristic.
But without it this algorithm becomes Crash Fault Tolerant only, for our proposes it&amp;#39;s enough.</description>
    </item>
    
    <item>
      <title>Sawtooth Network Monitoring</title>
      <link>https://opssec.in/2019/1116/</link>
      <pubDate>Sat, 16 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://opssec.in/2019/1116/</guid>
      <description>Introduction I need to agree with Prometheus vision, instrument everything. Every library, subsystem and service should have at least a few metrics to give you a rough idea of how it is performing.
It&amp;#39;s easier with instrumentation to get a snapshot of your system state in a particular time, the one can discovery outliers and misbehaviors and if you need more details you always have debugging logs.
Saying that, in this post we will analyze how to monitor a Sawtooth Validator Node, using InfluxDB and Grafana.</description>
    </item>
    
    <item>
      <title>Gossip networks</title>
      <link>https://opssec.in/2019/1111/</link>
      <pubDate>Mon, 11 Nov 2019 02:00:00 +0000</pubDate>
      
      <guid>https://opssec.in/2019/1111/</guid>
      <description>Introduction This is based on Sawtooth 1.0.5 source code for Gossip and static peering.
%load_ext autoreload %autoreload 2 import logging logger = logging.getLogger() logger.setLevel(logging.DEBUG) logging.info(&amp;#34;Starting&amp;#34;) from sawtooth_validator.networking.handlers import PingHandler from sawtooth_validator.journal.responder import ResponderBlockResponseHandler from sawtooth_validator.journal.responder import BlockResponderHandler from sawtooth_validator.state.settings_cache import SettingsCache from sawtooth_validator.gossip.gossip_handlers import PeerRegisterHandler from sawtooth_validator.networking.handlers import ConnectHandler from sawtooth_validator.server.keys import load_identity_signer from sawtooth_validator.concurrent.threadpool import InstrumentedThreadPoolExecutor from sawtooth_validator.networking.handlers import AuthorizationTrustRequestHandler from sawtooth_validator.gossip.permission_verifier import PermissionVerifier from sawtooth_validator.journal.block_store import BlockStore from sawtooth_validator.</description>
    </item>
    
  </channel>
</rss>
