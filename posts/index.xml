<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on knabben</title>
    <link>https://knabben.github.io/posts/</link>
    <description>Recent content in Posts on knabben</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 28 Aug 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://knabben.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>SAP-CCO - Design for Organizational Complexity</title>
      <link>https://knabben.github.io/posts/awsprof/readme/</link>
      <pubDate>Sun, 28 Aug 2022 00:00:00 +0000</pubDate>
      
      <guid>https://knabben.github.io/posts/awsprof/readme/</guid>
      <description>Domain 1: Design for Organizational Complexity  Percentage: 12.5%  1 Determine cross-account authentication and access strategy for complex organizations.  Analyze the organizational structure  Evaluate the current authentication infrastructure  Analyze the AWS resources at an account level  Determine an auditing strategy for authentication and access
Identity &amp;amp; Federation IAM - Roles, Groups, Policies (Resources, types) STS to assume role
Identity Federation SAML 2.0 federation ADFS
Web Identity Federation Cognito Microsoft AD / AD Federation services - Active Directory integration</description>
    </item>
    
    <item>
      <title>Hybrid nodes with ClusterClass</title>
      <link>https://knabben.github.io/posts/cc/readme/</link>
      <pubDate>Sat, 30 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>https://knabben.github.io/posts/cc/readme/</guid>
      <description>Introduction This post will be quick one to show how build a hybrid cluster using ClusterClass.
The idea behind ClusterClass is simple: define the shape of your cluster once, and reuse it many times, abstracting the complexities and the internals of a Kubernetes cluster away. [1]
From the original CAEP: Cluster API does not expose a native way to provision multiple clusters of the same configuration. The ClusterClass object is supposed to act as a collection of template references which can be used to create managed topologies.</description>
    </item>
    
    <item>
      <title>Cluster API Provider for AWS (CAPA)</title>
      <link>https://knabben.github.io/posts/capa/readme/</link>
      <pubDate>Tue, 12 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>https://knabben.github.io/posts/capa/readme/</guid>
      <description>Introduction The Cluster API is a Kubernetes project to bring declarative, Kubernetes-style APIs to cluster creation, configuration, and management. You can create new workload clusters from a management cluster in a descritive way, the ClusterAPI controllers provide the initial management cluster setup and a tool called clusterctl to manage them. Workloads clusters are specilized controllers on separated projects called providers. Each cloud provider will have it&#39;s own controllers and ways to build the workload cluster obeying the rules and logic defined in the spec and workflow diagrams here [2].</description>
    </item>
    
    <item>
      <title>Border Gateway Protocol (BGP) and Kubernetes</title>
      <link>https://knabben.github.io/posts/bgp/readme/</link>
      <pubDate>Fri, 10 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://knabben.github.io/posts/bgp/readme/</guid>
      <description>Introduction On this post I want to write a bit about Border Gateway Protocol, Calico and Kubernetes, in the examples we are going to see an use case with OpenBSD/openbgp and after Project Calico with BGP mode to illustrate how this protocol can be used in the CNI to provide automatic routing across nodes.
After reading leave comments in the end and share the post. Nuff said.
BGP protocol The RFC of BGP-4 is pretty clear in the abstract about the primary function of BGP [1] being is to exchange network reachability information with other BGP systems.</description>
    </item>
    
    <item>
      <title>Cluster API Provider for Azure (CAPZ)</title>
      <link>https://knabben.github.io/posts/capz/readme/</link>
      <pubDate>Sun, 20 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>https://knabben.github.io/posts/capz/readme/</guid>
      <description>Introduction From the official documentation [1] we have the original definition of Cluster API, it brings a declarative, Kubernetes-style APIs to cluster creation, configuration and management. The API itself is shared across multiple cloud providers allowing for true Azure hybrid deployments of Kubernetes.
Target Cluster Diagram This cluster was generated using the make create-workload-cluster from the CAPZ repository.
The diagram describes the CRDs [2] and objects created related to this Workload cluster inside Azure.</description>
    </item>
    
    <item>
      <title>KPNG Windows Userspace</title>
      <link>https://knabben.github.io/posts/kpng-proxy/readme/</link>
      <pubDate>Sat, 22 Jan 2022 00:00:00 +0000</pubDate>
      
      <guid>https://knabben.github.io/posts/kpng-proxy/readme/</guid>
      <description>Introduction KPNG (KubeProxy Next Generation, kproxy v2) has as a goal provide a more scalable version of service proxies on Kubernetes, the idea is provide a core system that comunicates with the API server and provides an SHIM interface for the backends. Backends are specialized code that implement the required steps to provide the load balancing for the endpoints based on the data plane technology of choice (iptables, ipvs, HCN, etc.</description>
    </item>
    
    <item>
      <title>Kube-proxy Windows Kernelspace Mode</title>
      <link>https://knabben.github.io/posts/winkernel/readme/</link>
      <pubDate>Sat, 18 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://knabben.github.io/posts/winkernel/readme/</guid>
      <description>Introduction Sorry not sorry, this is the fourth post about kube-proxy modes, don&#39;t worry the evil is in the details. This time these writings will cover Windows kernelspace mode. This proxy mode is used in Calico and Flannel CNIs and the APIs and golang binding used here (HCN) are shared on both technologies, so having a good understanding of how the public-facing interaction works and methods signatures is a good step on enabling the coding on windows container kernel networking capabilities.</description>
    </item>
    
    <item>
      <title>Kube-proxy Linux Userspace Mode</title>
      <link>https://knabben.github.io/posts/kproxy-modes/readme/</link>
      <pubDate>Sat, 27 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>https://knabben.github.io/posts/kproxy-modes/readme/</guid>
      <description>Introduction The following post continues a deep dive into kube-proxy modes, taking another approach to tackle the complexity that lies on this kind of system, loop diagrams are an interesting one, and we have used this to analyze the information flow in the Windows userspace mode. To have another holistic view of the entire mode and understand how these parts are interconnected can be a hard task in distributed systems, this is true not only for systems running on different machines, but for ones with parallel and concurrent mechanisms as well.</description>
    </item>
    
    <item>
      <title>Kube-proxy Winuserspace Mode</title>
      <link>https://knabben.github.io/posts/winuserspace/readme/</link>
      <pubDate>Sun, 17 Oct 2021 00:00:00 +0000</pubDate>
      
      <guid>https://knabben.github.io/posts/winuserspace/readme/</guid>
      <description>Introduction This post will introduce and detail the windows userspace mode on kube-proxy, the code walkthrough provided here has the goal to clarify the event based architecture of kube-proxy, and bring attention to how this component watches and reacts for events happening on Services and Endpoints API objects. This component goal is to proxy and load-balance the traffic to a binded IP/PORT pair across different backend services. Read more about Kube-Proxy in the official docs [1].</description>
    </item>
    
    <item>
      <title>Windows and Calico overlay networking</title>
      <link>https://knabben.github.io/posts/vxlan-windows/readme/</link>
      <pubDate>Fri, 03 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>https://knabben.github.io/posts/vxlan-windows/readme/</guid>
      <description>Introduction Windows are nowadays a must in a lot of multi-os clusters around companies, all big providers support the creation of Windows server machines and the support of containers in this OS is getting stronger on a daily basis [1].
The Kubernetes community have the Windows Special Interest Group [2], focusing on workloads and scheduling support for these Windows nodes. As support tooling and sub-projects around the main SIGs on K8s, a few experimentations are born like the Dev-Tools [3].</description>
    </item>
    
    <item>
      <title>CNI plugins and kindnet</title>
      <link>https://knabben.github.io/posts/cni-plugins/readme/</link>
      <pubDate>Sat, 28 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://knabben.github.io/posts/cni-plugins/readme/</guid>
      <description>Introduction With the advance of Containers, the network segmentation was required at level of the isolated applications, logically it&#39;s another copy of the network stack, with it&#39;s own routes, firewall rules and net devices. By convention these named network namespaces (ns) is an object at /var/run/netns/ as we are going to see in the sequence. A few tools to manage netns at user-space level resides on [1].
A standard called CNI (Container Network Interface) was created, to formalize and create a baseline that consists of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins.</description>
    </item>
    
    <item>
      <title>Kube-proxy ClusterIP</title>
      <link>https://knabben.github.io/posts/kproxy/readme/</link>
      <pubDate>Mon, 14 Jun 2021 00:00:00 +0000</pubDate>
      
      <guid>https://knabben.github.io/posts/kproxy/readme/</guid>
      <description>Introduction On this post I will bring a few details around Kube-proxy codebase and, how the rules are setup under the --mode=iptables and a few debugging commands to illustrate all the packet paths and endpoints. There&#39;s a simple cluster with 3 nodes on v1.21.1 created with Kind and kindnet as CNI.
Setup These are the following ips:
$ kubectl get nodes -A -o wide [21:42:09] NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME cluster-control-plane Ready control-plane,master 26h v1.</description>
    </item>
    
    <item>
      <title>Istio drill down - Part I</title>
      <link>https://knabben.github.io/posts/istio/readme/</link>
      <pubDate>Sat, 06 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://knabben.github.io/posts/istio/readme/</guid>
      <description>Introduction This post is a review and notes of some Istio tutorials around with the latest version, again the base cluster is created by kind and is being used as a CNI.
NAMESPACE NAME READY STATUS RESTARTS AGE kube-system calico-kube-controllers-5c6f6b67db-8rx2s 1/1 Running 0 4h26m kube-system calico-node-9fccq 1/1 Running 0 4h26m kube-system calico-node-b77sb 1/1 Running 0 4h26m kube-system calico-node-jf4z2 1/1 Running 0 4h26m kube-system calico-node-sl9qd 1/1 Running 0 4h26m kube-system coredns-74ff55c5b-dbwpl 1/1 Running 0 4h27m kube-system coredns-74ff55c5b-gpvz7 1/1 Running 0 4h27m kube-system etcd-calico-control-plane 1/1 Running 0 4h27m kube-system kube-apiserver-calico-control-plane 1/1 Running 0 4h27m kube-system kube-controller-manager-calico-control-plane 1/1 Running 0 4h27m kube-system kube-proxy-cc8sr 1/1 Running 0 4h27m kube-system kube-proxy-gbkl6 1/1 Running 0 4h27m kube-system kube-proxy-j2blv 1/1 Running 0 4h27m kube-system kube-proxy-jp9xm 1/1 Running 0 4h27m kube-system kube-scheduler-calico-control-plane 1/1 Running 0 4h27m local-path-storage local-path-provisioner-78776bfc44-m7kpq 1/1 Running 0 4h27m Installation This workshop one is made for GCP, but we are using a local cluster with Kind, Bypass old installation methods, and use the latest installation on v1.</description>
    </item>
    
    <item>
      <title>Kubernetes Windows Nodes</title>
      <link>https://knabben.github.io/posts/netpol-win/readme/</link>
      <pubDate>Sun, 28 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://knabben.github.io/posts/netpol-win/readme/</guid>
      <description>Introduction This post is a walkthrough on using a Windows node to run network policies with agnhost:2.26 mappings in order to understand how suitable the netpol suite is ready for it.
https://github.com/kubernetes/kubernetes/tree/master/test/e2e/network/netpol
Setting up the cluster Our master needs to be on Linux, so a GCP Vm with COS is being used, following up the quickstart on this node:
https://docs.projectcalico.org/getting-started/kubernetes/quickstart
Start the cluster in the following subnet with Kubeadm:
sudo kubeadm init --pod-network-cidr=192.</description>
    </item>
    
    <item>
      <title>Network Policy E2E test suite</title>
      <link>https://knabben.github.io/posts/netpol/readme/</link>
      <pubDate>Sun, 27 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://knabben.github.io/posts/netpol/readme/</guid>
      <description>Network policy This post documents a few steps to test the E2E framework using the Kind cluster with different CNIs, giving the developer some good tooling for debugging and quick test replication.
Use this script to bring a Kind cluster with a specific CNI setup.
Finding the config parameters In the ~/.kube/config you can find the Cluster sections host, use the IP and Port to start the tests.
apiVersion: v1 clusters: - cluster: certificate-authority-data: .</description>
    </item>
    
    <item>
      <title>Kubelet Dynamic Configuration</title>
      <link>https://knabben.github.io/posts/kube-dynamic/</link>
      <pubDate>Sun, 24 May 2020 16:00:00 +0000</pubDate>
      
      <guid>https://knabben.github.io/posts/kube-dynamic/</guid>
      <description>Introduction   On Kubernetes 1.10+ a new way to configure Kubelet via YAML became v1beta1, the struct KubeletConfiguration become available via the –config flag. So most of the regular ones can be configured via this file. This approach is extensively used by kubeadm for setting up the installation as was noted in last post.
 The idea of this post now is to do a code walkthrough and take a look in some KEPs and designs for the subsystem that reload this configuration at flight giving Kubelet a Dynamic Configuration capacity, this feature became beta on 1.</description>
    </item>
    
    <item>
      <title>Kubelet Debugging Session</title>
      <link>https://knabben.github.io/posts/kube-debug/</link>
      <pubDate>Sun, 10 May 2020 16:00:00 +0000</pubDate>
      
      <guid>https://knabben.github.io/posts/kube-debug/</guid>
      <description>Introduction   This post reflects some work on setting up a development environment for Kubernetes. We must have Goland, Virtualbox, Delve and MacOS installed. If you&amp;#39;re using Linux probably the vm part is not necessary.
  Installing the tools  VirtualBox   Since we are on MacOSX, and Kubelet nodes works only on Linux and Windows the setup of a vm is necessary for this step a Debian Buster image is going to be used, the Virtualbox is 6.</description>
    </item>
    
    <item>
      <title>Kubernetes Networking 101</title>
      <link>https://knabben.github.io/posts/kube-net/</link>
      <pubDate>Sat, 28 Dec 2019 23:00:00 +0000</pubDate>
      
      <guid>https://knabben.github.io/posts/kube-net/</guid>
      <description>Networking   The first and easiest way to access the Pod container port is via port-forward command:
 $ kubectl port-forward &amp;lt;pod_name&amp;gt; &amp;lt;local_port&amp;gt;:&amp;lt;remote_port&amp;gt;
 Take a look on the dynamic port-forwarder.
  Service   An abstract way to expose an application running on a set of Pods as a network service.
 a Service is an abstraction which defines a logical set of Pods and a policy by which to access them .</description>
    </item>
    
    <item>
      <title>Kubernetes Deployment Workloads</title>
      <link>https://knabben.github.io/posts/kube-depl/</link>
      <pubDate>Sat, 28 Dec 2019 11:00:00 +0000</pubDate>
      
      <guid>https://knabben.github.io/posts/kube-depl/</guid>
      <description>ReplicaSets   A ReplicaSet’s purpose is to maintain a stable set of replica Pods running at any given time. So, starting with the ReplicaSetSpec.
 // ReplicaSetSpec is the specification of a ReplicaSet. type ReplicaSetSpec struct { // Replicas is the number of desired replicas. 	// More info: https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/#what-is-a-replicationcontroller 	Replicas *int32 `json:&amp;#34;replicas,omitempty&amp;#34;` // Minimum number of seconds for which a newly created pod should be ready without any of its container crashing, for it to be considered available.</description>
    </item>
    
    <item>
      <title>Kubernetes Container API</title>
      <link>https://knabben.github.io/posts/kube-container/</link>
      <pubDate>Sat, 28 Dec 2019 10:00:00 +0000</pubDate>
      
      <guid>https://knabben.github.io/posts/kube-container/</guid>
      <description>Container from PodSpec   The abstraction of the container is find inside a list of Container objects inside the PodSpec, it means the possibility to create multiple containers in a unique Pod.
 // A single application container that you want to run within a pod. type Container struct { // Name of the container specified as a DNS_LABEL. 	Name string `json:&amp;#34;name&amp;#34;` // Docker image name. 	// More info: https://kubernetes.</description>
    </item>
    
    <item>
      <title>Pod specification</title>
      <link>https://knabben.github.io/posts/kube-pod/</link>
      <pubDate>Fri, 27 Dec 2019 15:00:00 +0000</pubDate>
      
      <guid>https://knabben.github.io/posts/kube-pod/</guid>
      <description>Introduction   Lets take a look in the default Kind definition of the Pod, with this we can enumerate all the fields and attached capabilities of the Pod. For this post only the main Spec of the Pod object is going to be detailed with examples and tasks, we let the container configuration for other posts. Get after it.
  POD diagram    Pod struct</description>
    </item>
    
    <item>
      <title>Kubectl Walkthrough</title>
      <link>https://knabben.github.io/posts/kube-ctl/</link>
      <pubDate>Thu, 26 Dec 2019 15:00:00 +0000</pubDate>
      
      <guid>https://knabben.github.io/posts/kube-ctl/</guid>
      <description>Introduction   This is swiss-knife of the Kubernetes operator, the kubectl is a tools for translating declarative resources in Kubernetes objects via the API, so the operator can load, remove or update workloads, configurations and more.
 The main source of truth is the Kubectl book but this post will try to summarize and organize the main capabilities of the tool.
  Kubectl debugging and API server   Kubectl is a client for the master API server, you can confirm this fact settings the verbose to 8, and you can fetch both the request and the response.</description>
    </item>
    
    <item>
      <title>Kubernetes in Docker</title>
      <link>https://knabben.github.io/posts/kind/</link>
      <pubDate>Wed, 25 Dec 2019 22:00:00 +0000</pubDate>
      
      <guid>https://knabben.github.io/posts/kind/</guid>
      <description>Introduction   This post is for discussing more about Kind, setup some cluster examples with some particular customizations, and exlain what components it brings up.
 The initial topics are a summary from Kind quickstart
  Installing   To install Kind you should use:
 $ brew install kind
 Or from source:
 $ GO111MODULE=&amp;#34;on&amp;#34; go get sigs.k8s.io/kind@v0.6.1
  First steps   To create a new cluster you must run:</description>
    </item>
    
    <item>
      <title>Sawtooth Cheatsheet</title>
      <link>https://knabben.github.io/posts/sawtooth-cheatsheet/</link>
      <pubDate>Sun, 15 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://knabben.github.io/posts/sawtooth-cheatsheet/</guid>
      <description>Installation  Ubuntu   For local ubuntu Installation is being used Ubuntu Xenial - 16.04 LTS, via vagrant
 $ vagrant init ubuntu/xenial64 $ vagrant up
 After the box is up we need to add the correct repository:
 $ sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys 8AA7AF1F1091A5FD $ sudo add-apt-repository &amp;#39;deb http://repo.sawtooth.me/ubuntu/1.0/stable xenial universe&amp;#39; $ sudo apt update
 Install the package:
 $ sudo apt install -y sawtooth</description>
    </item>
    
    <item>
      <title>Sawtooth Permissioning</title>
      <link>https://knabben.github.io/posts/sawtooth-permission/</link>
      <pubDate>Sun, 08 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://knabben.github.io/posts/sawtooth-permission/</guid>
      <description>Introduction   Sawtooth has two different kinds of permissioning, the first is the transaction permissioning, it controls who (user/clients) can submit transactions and batches to a validator, the second is validator key permissioning, this controls which nodes can connect to the Sawtooth network, only available with on-chain settings.
 For this debugging and testing scenario we will have 2 nodes:
 version: &amp;#34;2.1&amp;#34; services: v1: image: hyperledger/sawtooth-validator:1.1 container_name: v1 command: bash expose: - 8000 v2: image: hyperledger/sawtooth-validator:1.</description>
    </item>
    
    <item>
      <title>Kubernetes Audit Sink</title>
      <link>https://knabben.github.io/posts/kube-audit/</link>
      <pubDate>Wed, 27 Nov 2019 10:00:00 +0000</pubDate>
      
      <guid>https://knabben.github.io/posts/kube-audit/</guid>
      <description>Introduction   The idea of this post is to test the Audit dynamic backend with a KIND cluster.
 Kubernetes has 4 stages:
RequestReceived - The stage for events generated as soon as the audit handler receives the request, and before it is delegated down the handler chain.   ResponseStarted - Once the response headers are sent, but before the response body is sent. This stage is only generated for long-running requests (e.</description>
    </item>
    
    <item>
      <title>Kubernetes API client</title>
      <link>https://knabben.github.io/posts/kube-api/</link>
      <pubDate>Sat, 23 Nov 2019 10:00:00 +0000</pubDate>
      
      <guid>https://knabben.github.io/posts/kube-api/</guid>
      <description>Introduction   This is an overview and notes on how to use the client-go Kubernetes API.
  Kubernetes API resources  Accessing via API   The API is versioned for extensability propose, the levels are Alpha level (v1alpha1), Beta (b2beta3) stabel (v1).
 You can notice the groups of the resources exists inside the version. To access the endpoints directly start the proxy with  $ kubectl proxy --port=8080 $ curl http://localhost:8080/apis/ # To dig deeper in versions and existent resources $ kubectl api-resources &amp;amp;&amp;amp; kubectl api-version</description>
    </item>
    
    <item>
      <title>From Dev to PoET</title>
      <link>https://knabben.github.io/posts/dev-poet/</link>
      <pubDate>Sat, 16 Nov 2019 01:00:00 +0000</pubDate>
      
      <guid>https://knabben.github.io/posts/dev-poet/</guid>
      <description>Introduction   DevMode is very cool for development of transaction processors, but when you go to production it don&amp;#39;t grant a correct fork resolution of your chain, and you can end up with different HEAD in the nodes.
 Some Byzantine Fault Tolerant protocols were created to fix this problem, in version 1.0.5 we have the famous PoET, when used with SGX (intel protection) it can guarantee the BFT characteristic.</description>
    </item>
    
    <item>
      <title>Sawtooth Network Monitoring</title>
      <link>https://knabben.github.io/posts/sawtooth-grafana/</link>
      <pubDate>Sat, 16 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://knabben.github.io/posts/sawtooth-grafana/</guid>
      <description>Introduction   I need to agree with Prometheus vision, instrument everything. Every library, subsystem and service should have at least a few metrics to give you a rough idea of how it is performing.
 It&amp;#39;s easier with instrumentation to get a snapshot of your system state in a particular time, the one can discovery outliers and misbehaviors and if you need more details you always have debugging logs.</description>
    </item>
    
    <item>
      <title>Gossip networks</title>
      <link>https://knabben.github.io/posts/gossip/</link>
      <pubDate>Mon, 11 Nov 2019 02:00:00 +0000</pubDate>
      
      <guid>https://knabben.github.io/posts/gossip/</guid>
      <description>Introduction   This is based on Sawtooth 1.0.5 source code for Gossip and static peering.
  %load_ext autoreload %autoreload 2 import logging logger = logging.getLogger() logger.setLevel(logging.DEBUG) logging.info(&amp;#34;Starting&amp;#34;) from sawtooth_validator.networking.handlers import PingHandler from sawtooth_validator.journal.responder import ResponderBlockResponseHandler from sawtooth_validator.journal.responder import BlockResponderHandler from sawtooth_validator.state.settings_cache import SettingsCache from sawtooth_validator.gossip.gossip_handlers import PeerRegisterHandler from sawtooth_validator.networking.handlers import ConnectHandler from sawtooth_validator.server.keys import load_identity_signer from sawtooth_validator.concurrent.threadpool import InstrumentedThreadPoolExecutor from sawtooth_validator.networking.handlers import AuthorizationTrustRequestHandler from sawtooth_validator.gossip.permission_verifier import PermissionVerifier from sawtooth_validator.</description>
    </item>
    
    <item>
      <title>Golang essays (part II)</title>
      <link>https://knabben.github.io/posts/gop2/</link>
      <pubDate>Wed, 07 Aug 2019 01:00:00 +0000</pubDate>
      
      <guid>https://knabben.github.io/posts/gop2/</guid>
      <description>Pointers   A variable is a piece of storage containing a value. A pointer value is the memory address of a variable.
 The type *T is a pointer to a T. The operator that generates the pointer is the &amp;amp;.
 x := 1 p := &amp;amp;x // p, of type *int, points to x  fmt.Println(*p) // &amp;#34;1&amp;#34;  *p = 2 // dereferencing or indirecting  fmt.</description>
    </item>
    
    <item>
      <title>Golang essays (part I)</title>
      <link>https://knabben.github.io/posts/gop1/</link>
      <pubDate>Wed, 07 Aug 2019 00:00:00 +0000</pubDate>
      
      <guid>https://knabben.github.io/posts/gop1/</guid>
      <description>Introduction   Starting with the printed Hello World, create a main.go file with the following content:
 package main import ( &amp;#34;fmt&amp;#34; &amp;#34;math/rand&amp;#34; ) func main() { fmt.Println(&amp;#34;Hello, world\n&amp;#34;, rand.Intn(10)) } 
 On line 1 we have the package our programming is running. We import built-in packages on line 2, 3, 4. By convention, the package name is the same as the last element of the imported path, i.</description>
    </item>
    
    <item>
      <title>Rust 101</title>
      <link>https://knabben.github.io/posts/langs2/</link>
      <pubDate>Thu, 04 Jul 2019 08:00:00 -0400</pubDate>
      
      <guid>https://knabben.github.io/posts/langs2/</guid>
      <description>Introduction   To keep the research in , I started some basic repository with the some official Rust book code compilation, I will try in this post comment some of the features and tricks I got while reading it:
https://github.com/knabben/spec-lang/tree/master/go   https://github.com/knabben/spec-lang/tree/master/rust     Guess  Tag 0.0.1   println! is used to send the value to stdout.
 let mut guess = String::new();</description>
    </item>
    
    <item>
      <title>Hyperledger Sawtooth</title>
      <link>https://knabben.github.io/posts/sawtooth/</link>
      <pubDate>Sun, 02 Jun 2019 16:50:00 -0400</pubDate>
      
      <guid>https://knabben.github.io/posts/sawtooth/</guid>
      <description>Sawtooth   A RUST permissioned/permissionless distribuded ledger created by Intel, an official project under the Hyperledger umbrella.  From the characteristics of the sawtooth we can have: distributed, immutable, secure.
 There&amp;#39;s a separation between the application level and the core system. It provides smartcontract abstraction that allows application developers to write contract logic in a language of their choice.
 An application can be a native business logic or a smart contract virtual machine.</description>
    </item>
    
    <item>
      <title>Bootstraping languages</title>
      <link>https://knabben.github.io/posts/langs1/</link>
      <pubDate>Mon, 27 May 2019 14:04:34 -0400</pubDate>
      
      <guid>https://knabben.github.io/posts/langs1/</guid>
      <description>Long time I don&amp;#39;t write here.
 And, as a matter to learn other languages I&amp;#39;m going to share here some notes while in this path, giving the appropriate (or not) comments for each the steps taken in the journey.  As a good start let&amp;#39;s grab two modern interesting languages,
 The summary bellow came from wikipedia an works as a good introduction about both:
Summary  Golang   A statically typed, compiled programming language designed at Google by Robert Griesemer, Rob Pike, and Ken Thompson.</description>
    </item>
    
    <item>
      <title>Service Mesh</title>
      <link>https://knabben.github.io/posts/sm/</link>
      <pubDate>Sat, 06 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>https://knabben.github.io/posts/sm/</guid>
      <description>Introducing Kubernetes   November 2015, I started to use containers in production, and found some crazy problems involved with this in the process, it went from service discovery to persistent storage in a few days. The environment I used were basically a CoreOS cluster with some peculiarities, like starting services remote with fleet (like kubectl). The setup were composed of a etcd/fleet cluster with 3 machines and units being deployed with an external FLEETCTL_TUNNEL variable.</description>
    </item>
    
    <item>
      <title>Technical indicators</title>
      <link>https://knabben.github.io/posts/technical/</link>
      <pubDate>Sun, 25 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>https://knabben.github.io/posts/technical/</guid>
      <description>Introduction   According to investopedia - are mathematical calculations based on the price, volume, or open interest of a security or contract. So by analyzing historical data, technical analysts use indicators to predict the future price movements.
 The idea of this post is to show up how to calculate the most common one using the so famous SciPy stack, this is far from being an advising in this field, but a playground and notes about the research.</description>
    </item>
    
    <item>
      <title>More statistics and Python</title>
      <link>https://knabben.github.io/posts/statsmore/</link>
      <pubDate>Thu, 12 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>https://knabben.github.io/posts/statsmore/</guid>
      <description>Simple linear regression  Introduction   This is again a collection of notes about stats and finance, with focus on covariance, correlation and simple linear regression, the idea here is to analyse some tools to measure risk. For a better representation let&amp;#39;s get some Bovespa companies from the sector of Cyclic Consumption, they are: MGLU3, BTOW3, LAME3, LREN3.
Getting financial data   import pandas as pd import numpy as np import pandas_datareader as web import matplotlib.</description>
    </item>
    
    <item>
      <title>Probability</title>
      <link>https://knabben.github.io/posts/prob/</link>
      <pubDate>Sat, 01 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://knabben.github.io/posts/prob/</guid>
      <description>Sample space   An experiment is a process that, when performed, results in one and only one of many observations. These observations are called outcomes of the experiment. The collection of all outcomes for an experiment is called a sample space.
 | Experiment | Outcomes | Sample Space | |------------------|-------------|-------------------| | Roll a die | 1,2,3,4,5,6 | S = {1,2,3,4,5,6} | | Tooss a coin once| Head, Tail | S = {Head, Tail} |</description>
    </item>
    
    <item>
      <title>Statistics and Python</title>
      <link>https://knabben.github.io/posts/stats/</link>
      <pubDate>Sun, 25 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>https://knabben.github.io/posts/stats/</guid>
      <description>Descriptive Measures  Measures of Central tendency for Ungrouped data  Mean   Or the arithimetic mean, is the most frequently used measure of central tendency, the mean is the sum of all values / number of values.
 import numpy as np np.mean([1, 3, 6, 10, 20, 50]) # 15.0
  Median   Is the value of the middle term in a dataset that has been ranked in increasing order.</description>
    </item>
    
    <item>
      <title>Android APK hacking</title>
      <link>https://knabben.github.io/posts/apk/</link>
      <pubDate>Fri, 16 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>https://knabben.github.io/posts/apk/</guid>
      <description>Introduction   The post today is about fun, Kali Linux and Android apps hacking. Check it out!
 Why? Because people hide your access and don&amp;#39;t provide you a plain, well structured and easy to use API, no SDK, nothing. You are probalby locked in those apps, that normally share less than what you want.
  Host enumeration   The good old nmap should do the trick, find your network subnet, scan your hosts and look for obvious MAC address like Samsung Eletronics, for this post propouse we do have access to the cellphone, so obviously this step is not necessary.</description>
    </item>
    
    <item>
      <title>Continuous Deployment and DEIS</title>
      <link>https://knabben.github.io/posts/cddeis/</link>
      <pubDate>Sun, 27 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>https://knabben.github.io/posts/cddeis/</guid>
      <description>Introduction   Continuous Deployment and Continuous Integration are topics that you can evolve and improve forever. The best reading on it IMO is [1] by Jez Humble, and it will be the guide for this post.
 &amp;#34;The time from deciding that you need to make a change to having it in production is known as the cycle time, and it is a vital metric for any project.&amp;#34;</description>
    </item>
    
    <item>
      <title>Ecto vs. Django ORM</title>
      <link>https://knabben.github.io/posts/ecto/</link>
      <pubDate>Sat, 18 Jun 2016 00:00:00 +0000</pubDate>
      
      <guid>https://knabben.github.io/posts/ecto/</guid>
      <description>Introduction   Django [1] is awesome, it applies a lot of design patterns in different layers. First of you have the model layer, or the abstraction layer (models), you can define models that maps to the tables of your database. We are going to see some features that the Django ORM offers.
 On the other side, Phoenix Framework [2] is a web framework written in Elixir, it applies the MVC pattern too, and have Ecto as a &amp;#34;language integrated query composition tool&amp;#34; and a database wrapper.</description>
    </item>
    
    <item>
      <title>CPython Debugging</title>
      <link>https://knabben.github.io/posts/cpython/</link>
      <pubDate>Sat, 16 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>https://knabben.github.io/posts/cpython/</guid>
      <description>Introduction   Python Weekly this week bring a very cool article [1] about GDB and Python, so I thought it was a great opportunity to get into Python internals, the idea here is to understand (in more details) how Python executes your scripts and how it calls code functions.
 The code is very simple, and uses the random module with two built-in functions (range and sorted), the question is: Can we via GDB see the stack of it?</description>
    </item>
    
    <item>
      <title>Golang vs. Python</title>
      <link>https://knabben.github.io/posts/gopython/</link>
      <pubDate>Sat, 23 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>https://knabben.github.io/posts/gopython/</guid>
      <description>Introduction   Everybody lives Python, the syntax is fast to get used to and you can write most of your code on a constant pattern if you follow the PEP8. Lots of libraries are available and the community is vibrant and enthusiastic with the evolution of the language.
 For this exercise, the goal will be create a piece of code on Python that does the same thing on Golang.</description>
    </item>
    
    <item>
      <title>Monitoring MySQL w/ Prometheus</title>
      <link>https://knabben.github.io/posts/mysql/</link>
      <pubDate>Sat, 16 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>https://knabben.github.io/posts/mysql/</guid>
      <description>Introduction   On the last post we started prometheus on a CoreOS with cloud-init scripts, lets automate a little more the project an initialize our CoreOS dockers with service discovery for all services and a complete metric gather from an external MySQL database.
 So we already have an infrastructure, we just need two more containers, the first one is the MySQL metrics exporter, and the other is Grafana with Prometheus configured.</description>
    </item>
    
    <item>
      <title>Consul - Service Discovery</title>
      <link>https://knabben.github.io/posts/consul/</link>
      <pubDate>Thu, 03 Sep 2015 18:00:00 +0000</pubDate>
      
      <guid>https://knabben.github.io/posts/consul/</guid>
      <description>Introduction   Consul is a Service Discovery system, on a microservice architecture for example, we have some external and internal services laying around. There is a complexity in the setup and orchestration of these services which are being setup and destroyed all the time.
 A long term solution to the problem is to use DNS, an address name resolver. It is a mature and larger used protocol, but we can find some issues nowadays: How can it know if the service is down or up?</description>
    </item>
    
    <item>
      <title>General finance formulas</title>
      <link>https://knabben.github.io/posts/finance/</link>
      <pubDate>Sun, 09 Aug 2015 00:00:00 +0000</pubDate>
      
      <guid>https://knabben.github.io/posts/finance/</guid>
      <description>Future value   import numpy as np import warnings warnings.filterwarnings(&amp;#34;ignore&amp;#34;)
 Fv represents the future value of an operation in a particular time.  $$Fv = Pv * (1 + i)^n$$
 def fv(pv, n, i): print(&amp;#39;{0:.2f}&amp;#39;.format(np.fv(i/100, n, 0, -pv))) fv(5000, 9, 4.5) 7430.48 fv(5000, 12, 1.5) 5978.09 fv(1000, 18, 1.25) 1250.58 fv(2800, 3, 3.5) 3104.41 fv(2000, 3, 3.5) 2217.44 fv(1000, 3, 3.5) 1108.72 fv(340, 2, 5) 374.85 fv(1000, 48, 2.</description>
    </item>
    
    <item>
      <title>Tsung Capacity Test</title>
      <link>https://knabben.github.io/posts/capacity/</link>
      <pubDate>Sun, 26 Jul 2015 00:00:00 +0000</pubDate>
      
      <guid>https://knabben.github.io/posts/capacity/</guid>
      <description>Introduction   In this post we are going to analyze a very interesting Erlang distributed load testing tool, called Tsung [1]. The cool things about this tool is the complete HTML report of each connection made.
 NOTE If you are interested on real traffic replication, consider this project [2].
  Configuration   On tsung.xml we start the test plan, you can include here the list of clients, servers, the number/load time, as other sessions definitions.</description>
    </item>
    
  </channel>
</rss>
